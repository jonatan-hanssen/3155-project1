\documentclass{article}
	\usepackage[utf8]{inputenc}
	\usepackage{mathtools}
	\usepackage{amssymb}
	\usepackage{amsmath}
	\usepackage{bm}
	\usepackage{float}
	\usepackage{listings}
	\usepackage{nicefrac}
	\usepackage{xcolor}
	\usepackage[T1]{fontenc}
	\usepackage{newtxmath,newtxtext}
	\author{Jonatan Hoffmann Hanssen \and Eric Emanuel Reber}
	\title{Project 1 FYS-STK3155}

\begin{document}
	\maketitle

\section*{Abstract}

\section*{Introduction}

This is done by assuming that we can model our observed values as a linear combination of continous functions of the input values. For example, we can assume that our observed values can be approximately modeled by different polynomial degrees of the input values, and try to find the linear coefficients which minimize the difference between our observed values and the values we predict. This approach is essentially 


\section{Method}
\subsection{Preprocessing: Creating the design matrix and normalizing our data}

We have two datasets, one synthetic and one real. Regardless of the source of the data, we create the design matrix in a similar manner, as the relation between our input data and our target values are the same for both datasets, that is, for each combination of $x$ and $y$, we have a height $z$. For the Franke function this height is no more than the height over the $xy$-plane, but for our real data, this value represents the elevation in meters above ocean level for a terrain in Norway. As we want to do a polynomial fit of the data, we create our design matrix such that each row $i$ has the following values:

\[ 1 \;\; x_i \;\; y_i \; x_i^2 \;\; x_i y_i \;\; y_i^2 \;\; ...\]


These must be prepared in different ways to give us the best results. For the Franke function, we claim that no normalization is needed, as we generate the data ourselves to be between $0$ and $1$. Given that we are doing a polynomial fit, every element in our design matrix will be a multiplication of $x^i$ and $y^i$ for different $i$'s and $j$'s, which, given our choice of range for $x$ and $y$ guarantees that every element is between $0$ and $1$. This 

\subsection{Linear regression}

In this paper, we use three forms of linear regression to approximate both a synthetic dataset (the so called Franke Function) and a real dataset comprised of elevation data of a landscape in Norway. We use th

\section*{a}

$\bm{y}$ is defined as the $f(\bm{x}) + \bm{\epsilon}$. This can be approximated as a matrix multiplication $X\bm{\beta}$
plus an error vector $\bm{\epsilon}$. This means that each element of the vector
$\bm{y}$ can be expressed as follows:

\[ y_i = \sum_j x_{ij}\beta_j + \epsilon_i \]

If we take the expectation value of this expression we get the following:

\[ E[y_i] = E\left[\sum_j x_{ij}\beta_j + \epsilon_i\right] \]
\[ E[y_i] = E\left[\sum_j x_{ij}\beta_j\right] + E[\epsilon_i] \]

However, the elements of $X$ are not stochastic, and neither are the elements of $\beta$
and therefore
the first expectation value is simply the sum itself. Furthermore, $\epsilon$ is 
explicitly defined as a normal distribution $N(0,\sigma^2)$, and will by definition have
the expectation value $0$. Therefore, we end up with the final expression:

\[ E[y_i] = \sum_j x_{ij}\beta_j = \bm{X}_{i,*}\bm{\beta}\]

We can use expectation values to calculate the variance as well:

\[ var[y_i] = E\left[(y_i - E[y_i])^2\right] \]
\[ = E\left[y_i^2 - 2E[y_i]y_i + E[y_i]^2\right] \]

Distributing the outer expectation value function:

\[ var[y_i] = E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]^2\right] \]
% \[ = E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]^2\right] \]
\[ = E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]\right]E\left[E[y]\right] \]

The result of calculating the expectation value is non-stochastic. This means that
$E[E[X]] = E[X]$. From this it follows that

\[ var[y_i] = E[y_i^2] - 2E[y_i]E[y_i] + E[y_i]E[y_i] \]
\[ = E[y_i^2] - E[y_i]E[y_i] \]
\[ = E[y_i^2] - E[y_i]^2 \]

The expectation value in the second summand has been proven to be equal 
to $\bm{X}_{i,*}\bm{\beta}$ above. We therefore have

\[ var[y_i] = E[y_i^2] - (\bm{X}_{i,*}\bm{\beta})^2 \]
\[ = E\left[(X_{i,*}\bm{\beta})^2 + X_{i,*}\bm{\beta}\epsilon_i + \epsilon_i^2\right] - (\bm{X}_{i,*}\bm{\beta})^2 \]
\[ = E\left[(X_{i,*}\bm{\beta})^2\right] + E[X_{i,*}\bm{\beta}\epsilon_i] + E[\epsilon_i^2] - (\bm{X}_{i,*}\bm{\beta})^2 \]

$X_{i,*}\bm{\beta}$ and $\epsilon_i$ are both scalars. Therefore the expectation value can be written as
$E[X_{i,*}\bm{\beta}]E[\epsilon_i]$. However, $E[\epsilon_i]$ is by definition $0$, because $\epsilon_i$ is
defined as a normal distribution of mean $0$ and variance $\sigma^2$. Furthermore, the expectation value of the
non-stochastic $(X_{i,*}\bm{\beta})^2$ is simply the expression itself. We therefore have

\[ var[y_i] = (X_{i,*}\bm{\beta})^2 + E[X_{i,*}\bm{\beta}]\cdot 0 + E[\epsilon_i^2] - (\bm{X}_{i,*}\bm{\beta})^2 \]
\[ = E[\epsilon_i^2] \]

We can prove that this is equal to the variance of $\epsilon_i$:

\[ E[\epsilon_i^2] = \frac{1}{n} \sum_i \epsilon_i^2\]
\[ var[\epsilon_i] = \frac{1}{n} \sum_i (\epsilon_i - \bar{\epsilon_i})^2\]
\[ var[\epsilon_i] = \frac{1}{n} \sum_i (\epsilon_i - 0)^2\]
\[ var[\epsilon_i] = \frac{1}{n} \sum_i \epsilon_i^2 = E[\epsilon_i^2] \]

And of course, we know that the variance of $\epsilon_i$ by definition is $\sigma^2$.

\[ var[y_i] = E[\epsilon_i^2] = var[\epsilon_i] = \sigma^2 \]

\section{b}

Proof of $E[\hat{\beta}] = \beta$
$$E[\hat{\beta}] = E[(X^{T}X)^{-1}X^{T}y]$$
Because we are assuming $(X^{T}X)^{-1}X^{T}$ to be deterministic, $E[(X^{T}X)^{-1}X^{T}] = (X^{T}X)^{-1}X^{T}$. Moreover, we know $E[y] = y$, and we can thus rewrite the equation as follows:
$$E[(X^{T}X)^{-1}X^{T}y] = (X^{T}X)^{-1}X^{T}y = \beta$$
$\blacksquare$
\\

Proof of $var(\beta) = \sigma^{2}(X^{T}X)^{-1}$
$$var(\beta) = var((X^{T}X)^{-1}X^{T}y)$$
Because we are assuming $(X^{T}X)^{-1}X^{T}$ to be deterministic and $y$ to be stochastic, we can rewrite the $var(\beta)$ as following:
$$var(\beta) = (X^{T}X)^{-1}X^{T}var(y)((X^{T}X)^{-1}X^{T})^{T}$$
$$ = (X^{T}X)^{-1}X^{T}var(y)(X^{T})^{T}((X^{T}X)^{-1})^{T}$$
Since $(X^{T})^{T} = X$ and $(X^{T}X)^{-1})^{T}$ = $(X^{T}X)^{T})^{-1} = (X^{T}X)^{-1}$
$$ = (X^{T}X)^{-1}X^{T}var(y)X(X^{T}X)^{-1} $$
We know var(y) = $\sigma^{2}$, and since it is a scalar it is commutative. Thus we may move it freely
$$ = \sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} $$
$$ = \sigma^{2}(X^{T}X)^{-1} $$
$\blacksquare$


\section{c}

\subsection{Proof of the bias-variance decomposition}

Given data $\bm{y} = f(\bm{x}) + \bm{\epsilon}$ and a OLS model $\bm{\tilde{y}} = X\bm{\beta}$, we define
the Mean Squared Error as follows

\[C(X,\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i - \tilde{y_i})^2\]

This is simply the expectation value of $(\bm{y} - \bm{\tilde{y}})^2$

\[ C(X,\beta) = E\left[(\bm{y} - \bm{\tilde{y}})^2\right] \]

To derive the bias variance decomposition, we now add $E[\bm{\tilde{y}}] - E[\bm{\tilde{y}}]$ 
as a sneaky trick. We will also replace $\bm{y}$ with our definition $\bm{y} = \bm{f} + \bm{\epsilon}$

\[ C(X,\beta) = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}] + E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2\right] \]
\[ = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2 + 2\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])(E[\bm{\tilde{y}}] - \bm{\tilde{y}})\right] +(E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2\right] \]
\[ = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2\right] + 2E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])(E[\bm{\tilde{y}}] - \bm{\tilde{y}})\right] + E\left[(E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2\right] \]

Let us focus our attention on the middle term first.

\[ 2E\left[(\bm{f} + \bm{\epsilon} + E[\bm{\tilde{y}}])(E[\bm{\tilde{y}}] - \bm{\tilde{y}})\right] \]

Expanding the multiplication:

\[ 2E\left[\bm{f}E[\bm{\tilde{y}}] + \bm{\epsilon}E[\bm{\tilde{y}}] - E[\bm{\tilde{y}}]E[\bm{\tilde{y}}] - \bm{f}\bm{\tilde{y}} - \bm{\epsilon}\bm{\tilde{y}} + E[\bm{\tilde{y}}]\bm{\tilde{y}}\right] \]

Distributing the expectation value:

\[ 2\left[E[\bm{f}E[\bm{\tilde{y}}]] + E[\bm{\epsilon}E[\bm{\tilde{y}}]] - E[E[\bm{\tilde{y}}]E[\bm{\tilde{y}}]] - E[\bm{f}\bm{\tilde{y}}] - E[\bm{\epsilon}\bm{\tilde{y}}] + E[E[\bm{\tilde{y}}]\bm{\tilde{y}}]\right] \]

We know that $\bm{\tilde{y}}$ and $\bm{f}$ are non-stochastic, and we know that the expectation value of $\bm{\epsilon}$ is $0$. We also know that 
$\bm{\epsilon}$ and $\bm{\tilde{y}}$ are independent, which means we can write $E[\bm{\epsilon}\bm{\tilde{y}}]$ as $E[\bm{\epsilon}]E[\bm{\tilde{y}}]$ We therefore have

\[ 2\left[\bm{f}\bm{\tilde{y}} + 0 - \bm{\tilde{y}}\bm{\tilde{y}} - \bm{f}\bm{\tilde{y}} - 0 + \bm{\tilde{y}}\bm{\tilde{y}}\right] = 0 \]

We then have

\[ C(X,\beta) = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2\right] + E\left[(E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2\right] \]

The second term is by definition the variance of model.

\[ C(X,\beta) = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2\right] + var[\bm{\tilde{f}}] \]

And now the final term

\[ E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2\right] \]

\[ = E\left[\bm{f}^2 + \bm{f}\bm{\epsilon} - \bm{f}E[\bm{\tilde{y}}] + \bm{f\epsilon} + \bm{\epsilon}^2 - \bm{\epsilon}E[\bm{\tilde{y}}] - \bm{f}E[\bm{\tilde{y}}] - \bm{\epsilon}E[\bm{\tilde{y}}] + E[\bm{\tilde{y}}]^2 \right] \]

After distributing the expectation value we get the following, keeping in mind that $E[\bm{f}] = E[\bm{y} - \bm{\epsilon}] = E[\bm{y}] - E[\bm{\epsilon}] = \bm{y}$

\[ \bm{y}^2 - 2\bm{y}\bm{\tilde{y}} + E[\bm{\tilde{y}}]^2 + \sigma^2\]

\[ (\bm{y} + E[\bm{\tilde{y}}]^2 + \sigma^2 \]

We see that the quadratic form is equal to $bias[\bm{\tilde{y}}]^2$. We can then conclude that the cost function can be rewritten as follows:

\[ C(X,\beta) = bias[\bm{\tilde{y}}]^2 + var[\bm{\tilde{f}}] + \sigma^2 \]



% The first term here represents the bias, the second term is equal to zero and the last term represents the variance.


\end{document}
