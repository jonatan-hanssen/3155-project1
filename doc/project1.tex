\documentclass{article}
	\usepackage[utf8]{inputenc}
	\usepackage{mathtools}
	\usepackage{amssymb}
	\usepackage{amsmath}
	\usepackage{bm}
	\usepackage{float}
	\usepackage{listings}
	\usepackage{nicefrac}
	\usepackage{xcolor}
	\usepackage[T1]{fontenc}
	\usepackage{newtxmath,newtxtext}
	\author{Jonatan Hoffmann Hanssen \and Eric Emanuel Reber}
	\title{Project 1 FYS-STK3155}

\begin{document}
	\maketitle

\section*{Abstract}

\section*{a}

We have defined $\bm{y}$ as a function of a matrix multiplication $X\bm{\beta}$
plus an error vector $\bm{\epsilon}$. This means that each element of the vector
$\bm{y}$ can be expressed as follows:

\[ y_i = \sum_j x_{ij}\beta_j + \epsilon_i \]

If we take the expectation value of this expression we get the following:

\[ E[y_i] = E\left[\sum_j x_{ij}\beta_j + \epsilon_i\right] \]
\[ E[y_i] = E\left[\sum_j x_{ij}\beta_j\right] + E[\epsilon_i] \]

However, the elements of $X$ are not stochastic, and neither are the elements of $\beta$
the first expectation value is simply the sum itself. Furthermore, $\epsilon$ is 
explicitly defined as a normal distribution $N(0,\sigma^2)$, and will by definition have
the expectation value $0$. Therefore, we end up with the final expression:

\[ E[y_i] = \sum_j x_{ij}\beta_j = \bm{X}_{i,*}\bm{\beta}\]

We can use expectation values to calculate the variance as well:

\[ var[y_i] = E\left[(y_i - E[y_i])^2\right] \]
\[ = E\left[y_i^2 - 2E[y_i]y_i + E[y_i]^2\right] \]

Distributing the outer expectation value function:

\[ var[y_i] = E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]^2\right] \]
\[ = E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]^2\right] \]
\[ = E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]\right]E\left[E[y]\right] \]

The result of calculating the expectation value is non-stochastic. This means that
$E[E[X]] = E[X]$. From this it follows that

\[ = E[y_i^2] - 2E[y_i]E[y_i] + E[y_i]E[y_i] \]
\[ = E[y_i^2] - E[y_i]E[y_i] \]
\[ = E[y_i^2] - E[y_i]^2 \]

The expectation value in the second summand has been proven to be equal 
to $\bm{X}_{i,*}\bm{\beta}$ above. We therefore have

\[ var[y_i] = E[y_i^2] - (\bm{X}_{i,*}\bm{\beta})^2 \]
\[ = E\left[(X_{i,*}\bm{\beta})^2 + X_{i,*}\bm{\beta}\epsilon_i + \epsilon_i^2\right] - (\bm{X}_{i,*}\bm{\beta})^2 \]
\[ = E\left[(X_{i,*}\bm{\beta})^2\right] + E[X_{i,*}\bm{\beta}\epsilon_i] + E[\epsilon_i^2] - (\bm{X}_{i,*}\bm{\beta})^2 \]

$X_{i,*}\bm{\beta}$ and $\epsilon_i$ are both scalars. Therefore the expectation value can be written as
$E[X_{i,*}\bm{\beta}]E[\epsilon_i]$. However, $E[\epsilon_i]$ is by definition $0$, because $\epsilon_i$ is
defined as a normal distribution of mean $0$ and variance $\sigma^2$. Furthermore, the expectation value of the
non-stochastic $(X_{i,*}\bm{\beta})^2$ is simply the expression itself. We therefore have

\[ var[y_i] = (X_{i,*}\bm{\beta})^2 + E[X_{i,*}\bm{\beta}]\cdot 0 + E[\epsilon_i^2] - (\bm{X}_{i,*}\bm{\beta})^2 \]
\[ = E[\epsilon_i^2] \]

We can prove that this is equal to the variance of $\epsilon_i$:

\[ E[\epsilon_i^2] = \frac{1}{n} \sum_i \epsilon_i^2\]
\[ var[\epsilon_i] = \frac{1}{n} \sum_i (\epsilon_i - \bar{\epsilon_i})^2\]
\[ var[\epsilon_i] = \frac{1}{n} \sum_i (\epsilon_i - 0)^2\]
\[ var[\epsilon_i] = \frac{1}{n} \sum_i \epsilon_i^2 = E[\epsilon_i^2] \]

And of course, we know that the variance of $\epsilon_i$ by definition is $\sigma^2$.

\[ var[y_i] = E[\epsilon_i^2] = var[\epsilon_i] = \sigma^2 \]

Proof of $E[\hat{\beta}] = \beta$
$$E[\hat{\beta}] = E[(X^{T}X)^{-1}X^{T}y]$$
Because we are assuming $(X^{T}X)^{-1}X^{T}$ to be deterministic, $E[(X^{T}X)^{-1}X^{T}] = (X^{T}X)^{-1}X^{T}$. Moreover, we know $E[y] = y$, and we can thus rewrite the equation as follows:
$$E[(X^{T}X)^{-1}X^{T}y] = (X^{T}X)^{-1}X^{T}y = \beta$$
$\blacksquare$
\\

Proof of $var(\beta) = \sigma^{2}(X^{T}X)^{-1}$
$$var(\beta) = var((X^{T}X)^{-1}X^{T}y)$$
Because we are assuming $(X^{T}X)^{-1}X^{T}$ to be deterministic and $y$ to be stochastic, we can rewrite the $var(\beta)$ as following:
$$var(\beta) = (X^{T}X)^{-1}X^{T}var(y)((X^{T}X)^{-1}X^{T})^{T}$$
$$ = (X^{T}X)^{-1}X^{T}var(y)(X^{T})^{T}((X^{T}X)^{-1})^{T}$$
Since $(X^{T})^{T} = X$ and $(X^{T}X)^{-1})^{T}$ = $(X^{T}X)^{T})^{-1} = (X^{T}X)^{-1}$
$$ = (X^{T}X)^{-1}X^{T}var(y)X(X^{T}X)^{-1} $$
We know var(y) = $\sigma^{2}$, and since it is a scalar it is commutative. Thus we may move it freely
$$ = \sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} $$
$$ = \sigma^{2}(X^{T}X)^{-1} $$
$\blacksquare$
\end{document}
