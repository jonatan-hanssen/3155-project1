\documentclass[twocolumn,10pt,cleanfoot]{asme2ej}

\usepackage{graphicx} %% for loading jpg figures
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{listings}
\usepackage{tablefootnote}
\usepackage{float}
\usepackage{xcolor}
\usepackage{hyperref}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{breaklines=true, style=mystyle}
%% The class has several options
%  onecolumn/twocolumn - format for one or two columns per page
%  10pt/11pt/12pt - use 10, 11, or 12 point font
%  oneside/twoside - format for oneside/twosided printing
%  final/draft - format for final/draft copy
%  cleanfoot - take out copyright info in footer leave page number
%  cleanhead - take out the conference banner on the title page
%  titlepage/notitlepage - put in titlepage or leave out titlepage
%  
%% The default is oneside, onecolumn, 10pt, final

\title{Linear regression for approximating real and synthetic data}

%%% first author
\author{Jonatan H. Hanssen
    \affiliation{
	Bachelor Student, Robotics and Intelligent Systems\\
	Department of Informatics\\
	The faculty of Mathmatics and Natural Sciences\\
    Email: jonatahh@ifi.uio.no
    }
}

\author{Eric E. Reber
    \affiliation{
	Bachelor Student, Robotics and Intelligent Systems\\
	Department of Informatics\\
	The faculty of Mathmatics and Natural Sciences\\
    Email: ericer@ifi.uio.no
    }
}


\begin{document}


\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Abstract}
We used linear regression to create a fit of a synthetic dataset generated by the Franke function, and of a real dataset of elevation data from a terrain in Norway. More specifically, we used Ordinary Least Squares (OLS), Ridge and Lasso regression to explore how the different methods perform on different datasets. On the synthetic dataset, we found that Ridge regression allowed us to increase our model complexity and achieve lower MSE scores than OLS, but that Lasso struggled due to computational constraints. On the real dataset, we found that regularization had little impact, as we did not reach model complexities in which overfitting occured.
% Comparing the Mean Squared Error (MSE) for the different methods, we found that Ridge and Lasso outperform OLS for small datasets and high polynomial degrees, but that OLS, given the optimal polynomial degree for the dataset, yields the lowest MSE. Overall we found that the different forms of linear regression fit the synthetic data well. However, when looking at the real datasets, we found that all forms of linear regression performed relatively poorly. We concluded that linear regression perhaps is not the best way of predicting terrain data with rapid and irregular changes in elevation.

% \tableofcontents
% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{nomenclature}
% \entry{A}{You may include nomenclature here.}
% \entry{$\alpha$}{There are two arguments for each entry of the nomemclature environment, the symbol and the definition.}
% \end{nomenclature}
%
% The primary text heading is  boldface and flushed left with the left margin.  The spacing between the  text and the heading is two line spaces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

One of the core challenges in machine learning is model selection, because we cannot make a priori assessments about how well the model will fit our dataset. A multitude of factors determine the efficacy of our model, the machine learning method we use, the hyperparameters of said method, and the model complexity we select. An uninformed decision in model selection could result in an under- or overfitting to the data, yielding poor results. Thus, it is important that we rigorously test our tuning such that these factors coincide at an intersection, providing the minimal error in unity.

In this paper, we will provide a detailed inspection of the Ordinary Least Squares (OLS), Ridge and Lasso linear regression methods, their hyperparameters and varying model complexities with the aim of making an informed decision about model selection when fitting to different datasets. The datasets consist of synthetic data created by the Franke function, and a real dataset of terrain in Norway. We inform our decisions by plotting the mean squared error (MSE), our cost function, over different polynomial degrees describing model complexity, over different resampling techniques and over different values of lambda, which is the hyperparameter responsible for a suitable regularization. We will perform a bias-variance decomposition for the different linear regression methods to check for under- and overfitting and plot our implementations of the methods up against popular implementations such as scikit learn to ensure that the behaviour of our models is expected.

First, we will go more into depth about the methods and the theory behind them, along with a brief description of our implementation. We will further continue to the results, alongside a critical discussion of their implications. Finally, we will give a conclusion on the optimal model selection for the different datasets.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Method}

\subsection{Theory}

This sections covers the theoretic background which underlies the methods used in this paper. We use the following shorthands for our mathematical notation.

\subsubsection{Linear regression}

To describe how linear regression works, let us first describe the problem it aims to solve. Assume we have a dataset, which contains a series of inputs $\{x_i\}$, and corresponding target values $\{y_i\}$. Our assumption is that there is some relationship $y_i = f(x_i) + \epsilon_i$ where $\epsilon_i \sim N(0,\sigma^2)$, and we wish to create a model which approximates this relationship. More specifically, we wish to find a vector $\beta$ which gives us the "best" approximation $\bm{\tilde{y}}$ of $\bm{y}$, by using the parameters in the following equation:

\begin{equation}
X \bm{\beta} = \bm{\tilde{y}}
\label{first}
\end{equation}


$X$ is called the design matrix, and is an $n$ by $p$ matrix created by our input data and our choice of basis functions, where $n$ is the amount of different input states we have and $p$ is the number of features we use for our fit. For our problem, $n$ is the number of distinct $x$ values times the number of distinct $y$ values, and $p$ is based on the polynomial degree we use. The basis functions are the summands of a two variable polynomial of degree $N$. The element $x_{ij}$ is calculated by evaluating the basis function corresponding the coloumn $j$ for the input data corresponding to the row $i$. As we can see from Eq.~\ref{first}, each row of this matrix is scalar multiplied by the vector $\beta$, giving us a linear combination of the values of $X_{i*}$ for each $y_i$ in the vector $\bm{\tilde{y}}$. The vector $\bm{\beta}$ is called the parameter vector, and finding the optimal values $\hat{\bm{\beta}}$ is the problem we wish to solve. But for this, we need to define what an "optimal" prediction $\bm{\tilde{y}}$ is.

\subsubsection{The cost function}

To define the optimal prediction, we use a cost function, and the choice of cost function greatly influences what optimal parameters we get.\footnote{It is here important to keep in mind that the cost function used when fitting the model (training) rarely is the same as the one used for testing our prediction} An intuitive cost function is the mean squared error (MSE). This function simply computes the average distance between our prediction and the target value, squared:

\begin{equation}
	C_{mse}(\tilde{\bm{y}}) = \sum_{i=0}^{n-1}(y_i-\tilde{y_i})^2
\end{equation}

We may now plug in the definition of $\bm{\tilde{y}}$.

\begin{equation}
	C_{mse}(\bm{\beta}) = \sum_{i=0}^{n-1}(y_i-X_{i*}\bm{\beta})^2
\end{equation}

We see that we have a function of our parameters $\bm{\beta}$, which describe how much our prediction deviates from our target values. Our problem now is to optimize this function, that is, find the values of $\beta$ which gives the MSE the lowest value. Luckily for us, it can be proven that the second derivative of this function is always positive \cite[106]{linreg}, which means that we can find the global minimum by solving the equation

\begin{gather}
\frac{\delta}{\delta \beta} \sum_{i=0}^{n-1}(y_i-X_{i*}\bm{\beta})^2 = 0 \\
 - \frac{2}{n} X^T(\bm{y} - X\bm{\beta}) = 0
\end{gather}

Solving this for $\beta$ gives us the so called normal equation, an analytical solution\footnote{This solution requires that $X^TX$ is non-singular, which may not be the case. This is solved by using the singular value decomposition of the design matrix, which we will not go into here} for the optimal $\hat{\beta}$.

% \begin{equation}
\begin{gather}
- \frac{2}{n} X^T(\bm{y} - X\bm{\beta}) = 0 \\
X^T(\bm{y} - X\bm{\beta}) = 0 \\
X^T\bm{y} - X^TX\bm{\beta} = 0 \\
X^T\bm{y} = X^TX\bm{\beta} \\
\bm{\hat{\beta}} = (X^TX)^{-1}X^T\bm{y}
\end{gather}

The process we have just described, with MSE as the cost function, is what is known as Ordinary Least Squares (OLS) regression. Using different cost functions when fitting the model leads to different equations for $\bm{\hat{\beta}}$. This leads us to the two next forms of linear regression we use in this paper. But before this, a...

\subsubsection{Brief statistical intermission}

We must remember that our target values are defined as $y_i = f + \epsilon_i$, that is, they are distorted by a stochastic noise $\epsilon_i \sim N(0,\sigma^2)$. We should therefore take a brief intermission to prove some of the statistical qualities of the method we have just described, so as to reassure ourselves that we are justified in applying linear regression to noisy data. We will start by showing that

\begin{equation}
E[y_i] = \sum_j X_{ij}\beta_j
\end{equation}

The proof is as follows:

$\bm{y}$ is defined as the $f(\bm{x}) + \bm{\epsilon}$. We make an assumption that this can be approximated as a matrix multiplication $X\bm{\beta}$ plus an error vector $\bm{\epsilon}$. This means that each element of the vector
$\bm{y}$ can be expressed as follows:

\begin{equation}
y_i = \sum_j x_{ij}\beta_j + \epsilon_i
\end{equation}

If we take the expectation value of this expression we get the following:

\begin{gather}
E[y_i] = E\left[\sum_j x_{ij}\beta_j + \epsilon_i\right] \\
E[y_i] = E\left[\sum_j x_{ij}\beta_j\right] + E[\epsilon_i]
\end{gather}

However, the elements of $X$ are not stochastic, and neither are the elements of $\beta$ and therefore the first expectation value is simply the sum itself. Furthermore, $\epsilon$ is explicitly defined as a normal distribution $N(0,\sigma^2)$, and will by definition have the expectation value $0$. Therefore, we end up with the final expression:

\begin{equation}
E[y_i] = \sum_j x_{ij}\beta_j = \bm{X}_{i,*}\bm{\beta}
\end{equation}

Next, we show that the variance of $y_i$ is equal to the variance of the noise:

\begin{gather}
var[y_i] = E\left[(y_i - E[y_i])^2\right]  \\
= E\left[y_i^2 - 2E[y_i]y_i + E[y_i]^2\right]
\end{gather}

Distributing the outer expectation value function:

\begin{gather}
var[y_i] = E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]^2\right]  \\
= E[y_i^2] - 2E\left[E[y_i]\right]E[y_i] + E\left[E[y]\right]E\left[E[y]\right]
\end{gather}

The result of calculating the expectation value is non-stochastic. This means that $E[E[X]] = E[X]$. From this it follows that

\begin{gather}
var[y_i] = E[y_i^2] - 2E[y_i]E[y_i] + E[y_i]E[y_i]  \\
= E[y_i^2] - E[y_i]E[y_i]  \\
= E[y_i^2] - E[y_i]^2
\end{gather}

The expectation value in the second summand has been proven to be equal to $\bm{X}_{i,*}\bm{\beta}$ above. We therefore have

\begin{gather}
var[y_i] = E[y_i^2] - (\bm{X}_{i,*}\bm{\beta})^2  \\
= E\left[(X_{i,*}\bm{\beta})^2 + X_{i,*}\bm{\beta}\epsilon_i + \epsilon_i^2\right] - (\bm{X}_{i,*}\bm{\beta})^2  \\
= E\left[(X_{i,*}\bm{\beta})^2\right] + E[X_{i,*}\bm{\beta}\epsilon_i] + E[\epsilon_i^2] - (\bm{X}_{i,*}\bm{\beta})^2
\end{gather}

$X_{i,*}\bm{\beta}$ and $\epsilon_i$ are both scalars. Therefore the expectation value can be written as $E[X_{i,*}\bm{\beta}]E[\epsilon_i]$. However, $E[\epsilon_i]$ is by definition $0$, because $\epsilon_i$ is defined as a normal distribution of mean $0$ and variance $\sigma^2$. Furthermore, the expectation value of the non-stochastic $(X_{i,*}\bm{\beta})^2$ is simply the expression itself. We therefore have

\begin{gather}
var[y_i] = (X_{i,*}\bm{\beta})^2 + E[X_{i,*}\bm{\beta}]\cdot 0 + E[\epsilon_i^2] - (\bm{X}_{i,*}\bm{\beta})^2  \\
= E[\epsilon_i^2]
\end{gather}

We can prove that this is equal to the variance of $\epsilon_i$:

\begin{gather}
E[\epsilon_i^2] = \frac{1}{n} \sum_i \epsilon_i^2 \\
var[\epsilon_i] = \frac{1}{n} \sum_i (\epsilon_i - \bar{\epsilon_i})^2 \\
var[\epsilon_i] = \frac{1}{n} \sum_i (\epsilon_i - 0)^2 \\
var[\epsilon_i] = \frac{1}{n} \sum_i \epsilon_i^2 = E[\epsilon_i^2]
\end{gather}

And of course, we know that the variance of $\epsilon_i$ by definition is $\sigma^2$.

\begin{gather}
var[y_i] = E[\epsilon_i^2] = var[\epsilon_i] = \sigma^2 
\end{gather}

We should also prove some qualities about or parameters. We start by proving that $E[\hat{\beta}] = \beta$

Because we are assuming $(X^{T}X)^{-1}X^{T}$ to be deterministic, $E[(X^{T}X)^{-1}X^{T}] = (X^{T}X)^{-1}X^{T}$. Moreover, we know $E[y] = y$, and we can thus rewrite the equation as follows:

\begin{equation}
E[(X^{T}X)^{-1}X^{T}y] = (X^{T}X)^{-1}X^{T}y = \beta
\end{equation}

Next, we prove that $var(\beta) = \sigma^{2}(X^{T}X)^{-1}$.

$$var(\beta) = var((X^{T}X)^{-1}X^{T}y)$$

Because we are assuming $(X^{T}X)^{-1}X^{T}$ to be deterministic and $y$ to be stochastic, we can rewrite the $var(\beta)$ as following:

\begin{gather}
var(\beta) = (X^{T}X)^{-1}X^{T}var(y)((X^{T}X)^{-1}X^{T})^{T} \\
= (X^{T}X)^{-1}X^{T}var(y)(X^{T})^{T}((X^{T}X)^{-1})^{T}
\end{gather}

Since $(X^{T})^{T} = X$ and $(X^{T}X)^{-1})^{T}$ = $(X^{T}X)^{T})^{-1} = (X^{T}X)^{-1}$

\begin{gather}
= (X^{T}X)^{-1}X^{T}var(y)X(X^{T}X)^{-1}
\end{gather}

We know var(y) = $\sigma^{2}$, and since it is a scalar it is commutative. Thus we may move it freely

\begin{gather}
= \sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\
= \sigma^{2}(X^{T}X)^{-1}
\end{gather}

\subsubsection{Ridge Regression}

A common problem with OLS is what is known as overfitting (we will discuss this in greater detail in later sections). A symptom of overfitting is that the values in the parameter vector $\bm{\beta}$ grow very large, and vary wildly for small changes in our dataset. To counteract this, we can add to the cost function a term which increases with the values in $\bm{\beta}$. Optimizing the cost function would now not only mean finding the smallest difference between target and predicted values, but also minimizing the absolute value of the parameters. We now define a new cost function

\begin{equation}
	C_{lasso}(\bm{\beta}) = \sum_{i=0}^{n-1}(y_i-X_{i*}\bm{\beta})^2 + \lambda \sum_{j=0}^{p-1} \beta_j^2
\end{equation}

$||\beta||_2^2$ is equal to $\beta^T\beta$ and is simply the norm of the parameter vector, squared. With the new hyperparameter $\lambda \in (0, \inf)$ , we can decide how harshly we should punish large values in our parameter vector. Finding an optimal $\lambda$ is an important step in the process of finding the best parameters for our model.

A similar derivation as done in the previous section gives us the normal equation for Ridge regression:

\begin{equation}
	\bm{\hat{\beta}} = (X^TX + \lambda I)^{-1}X^T\bm{y}
\end{equation}

We see that this equation, with a $\lambda > 0$ avoids the problem of an inversion of a non-singular matrix, and gives us an analytical solution for $\bm{\hat{\beta}}$ yet again.\footnote{We should cherish this analytical solution, as it may well be the last time we see a cost function that can be optimized analytically.} By using ridge regresison, we can control the complexity of our model and effectively hinder overfitting.

\subsubsection{Lasso Regression}

Another cost function is one where we add the L1-norm (the sum of the absolute values of the parameters), instead of the L2-norm. The new cost function is as follows.

\begin{equation}
	C_{lasso}(\bm{\beta}) = \sum_{i=0}^{n-1}(y_i-X_{i*}\bm{\beta})^2 + \lambda \sum_{j=0}^{p-1} |\beta_j|
\end{equation}

However, the derivative of the absolute value is not continous, so there is no analytical solution for the optimal parameters with this form of linear regression. We must then solve the problem numerically, which can be done by using Gradient Descent. Like Ridge regression, this form of regression controls the complexity of the model, with the added benefit that Lasso can drive parameters to exactly zero.

\subsubsection{Resampling methods: The bootstrap}

Bootstrap is a resampling method divided into simple steps. First, we reshuffle our dataset with replacement, which will provide us with a new dataset. Note that we will already have split our data into train and test data before we begin the bootstrap, so when we reshuffle, we will only reshuffle the training data. The reshuffled datasets act as our samples.

Secondly, we will perform our linear regression on this new dataset and store the predictions of the test data. Thirdly, we repeat for a given large number of iterations. Since we use all the test predictions to calculate the MSE, bias and variance, we will then according to the central limit theorem \cite{CLT} obtain a normal distribution of these performance metrics. The law of large numbers \cite{LLN} then tells us that the mean sample values of the MSE, bias and variance approach the true value of MSE, bias and variance. This is dependent on our assumption that our data is independently and identically distributed.

These approximations of the true value of MSE, bias and variance will become useful when performing our bias-variance trade-off analysis.


\subsubsection{Cross validation}

Cross validation is another resampling technique, in which we reshuffle our data and then split them into k-folds, where k specifies the number of folds. One fold is used as our test data, and the rest is used as training data. We run our linear regression, then switch up which fold is used for testing until all folds have been used. As in bootstrapping, we will approach the true values of for example the mean squared error for our model. 
Theoretically, because both two resampling techniques use the central limit theorem and the law of large numbers, they should return the same values for the MSE, bias and variance given the same dataset. Though resampling techniques can be computationally expensive, they make up for it by aiding in model evaluation by supplying us with performance metrics obtained from a larger number of samples. 

\subsubsection{Bias-variance trade-off}

A crucial part of model evaluation is ensuring that the model is neither underfitted nor overfitted, which we can determine by observing the relationship between the bias and the variance. Bias is defined as $(Bias[\tilde{y}])^2 = (y-E[\tilde{y}])^2$, the difference between the expected value of our prediction and the observed value. Models with high bias are usually underfitted, whilst models with low bias are the opposite. Variance on the other hand is defined as $var[\tilde{f}] = \frac{1}{n}\sum(\tilde{y_i}-E[\tilde{y}])^2$, how far away predictions are from the mean prediction. Models with high variance are usually overfitted to the training data, whilst low variance models are the opposite.

A good rule of thumb for finding a good fit is looking at the intercept between bias and variance. This is because the cost function can be decomposed into these two elements. The proof for this is as follows:

Given data $\bm{y} = f(\bm{x}) + \bm{\epsilon}$ and a OLS model $\bm{\tilde{y}} = X\bm{\beta}$, we define
the Mean Squared Error as follows

\begin{gather}
C(X,\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i - \tilde{y_i})^2
\end{gather}

This is simply the expectation value of $(\bm{y} - \bm{\tilde{y}})^2$

\begin{gather}
C(X,\beta) = E\left[(\bm{y} - \bm{\tilde{y}})^2\right]
\end{gather}

To derive the bias variance decomposition, we now add $E[\bm{\tilde{y}}] - E[\bm{\tilde{y}}]$ 
as a sneaky trick. We will also replace $\bm{y}$ with our definition $\bm{y} = \bm{f} + \bm{\epsilon}$

\begin{gather}
C(X,\beta) = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}] + E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2\right]
\end{gather}
%%%%%%%%%%%%%%%%%5
\begin{multline}
= E[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2 + 2[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])(E[\bm{\tilde{y}}] \\
	- \bm{\tilde{y}})] +(E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2]
\end{multline}

\begin{multline}
= E[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2] + \\
	2E[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])(E[\bm{\tilde{y}}] - \bm{\tilde{y}})] + E[(E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2]
\end{multline}

Let us focus our attention on the middle term first.

\begin{gather}
2E\left[(\bm{f} + \bm{\epsilon} + E[\bm{\tilde{y}}])(E[\bm{\tilde{y}}] - \bm{\tilde{y}})\right] 
\end{gather}

Expanding the multiplication:

\begin{gather}
2E\left[\bm{f}E[\bm{\tilde{y}}] + \bm{\epsilon}E[\bm{\tilde{y}}] - E[\bm{\tilde{y}}]E[\bm{\tilde{y}}] - \bm{f}\bm{\tilde{y}} - \bm{\epsilon}\bm{\tilde{y}} + E[\bm{\tilde{y}}]\bm{\tilde{y}}\right] 
\end{gather}

Distributing the expectation value:

\begin{gather}
2\left[E[\bm{f}E[\bm{\tilde{y}}]] + E[\bm{\epsilon}E[\bm{\tilde{y}}]] - E[E[\bm{\tilde{y}}]E[\bm{\tilde{y}}]] - E[\bm{f}\bm{\tilde{y}}] - E[\bm{\epsilon}\bm{\tilde{y}}] + E[E[\bm{\tilde{y}}]\bm{\tilde{y}}]\right] 
\end{gather}

We know that $\bm{\tilde{y}}$ and $\bm{f}$ are non-stochastic, and we know that the expectation value of $\bm{\epsilon}$ is $0$. We also know that 
$\bm{\epsilon}$ and $\bm{\tilde{y}}$ are independent, which means we can write $E[\bm{\epsilon}\bm{\tilde{y}}]$ as $E[\bm{\epsilon}]E[\bm{\tilde{y}}]$ We therefore have

\begin{gather}
2\left[\bm{f}\bm{\tilde{y}} + 0 - \bm{\tilde{y}}\bm{\tilde{y}} - \bm{f}\bm{\tilde{y}} - 0 + \bm{\tilde{y}}\bm{\tilde{y}}\right] = 0 
\end{gather}

We then have

\begin{gather}
C(X,\beta) = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2\right] + E\left[(E[\bm{\tilde{y}}] - \bm{\tilde{y}})^2\right] 
\end{gather}

The second term is by definition the variance of model.

\begin{gather}
C(X,\beta) = E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2\right] + var[\bm{\tilde{f}}] 
\end{gather}

And now the final term

\begin{gather}
E\left[(\bm{f} + \bm{\epsilon} - E[\bm{\tilde{y}}])^2\right]
\end{gather}

\begin{multline}
= E[\bm{f}^2 + \bm{f}\bm{\epsilon} - \bm{f}E[\bm{\tilde{y}}] + \bm{f\epsilon} + \bm{\epsilon}^2 - \\ 
	\bm{\epsilon}E[\bm{\tilde{y}}] - \bm{f}E[\bm{\tilde{y}}] - \bm{\epsilon}E[\bm{\tilde{y}}] + E[\bm{\tilde{y}}]^2]
\end{multline}

After distributing the expectation value we get the following, keeping in mind that $E[\bm{f}] = E[\bm{y} - \bm{\epsilon}] = E[\bm{y}] - E[\bm{\epsilon}] = \bm{y}$

\begin{gather}
\bm{y}^2 - 2\bm{y}\bm{\tilde{y}} + E[\bm{\tilde{y}}]^2 + \sigma^2 \\
(\bm{y} + E[\bm{\tilde{y}}]^2 + \sigma^2 
\end{gather}

We see that the quadratic form is equal to $bias[\bm{\tilde{y}}]^2$. We can then conclude that the cost function can be rewritten as follows:

\begin{gather}
C(X,\beta) = bias[\bm{\tilde{y}}]^2 + var[\bm{\tilde{f}}] + \sigma^2 
\end{gather}


Because the cost function is a sum of the bias and the variance, the lowest error will generally occur when both bias and variance are low. This also fits our understanding of the definitions of bias and variance, as we wish to obtain a model that is neither high in bias (underfitted) or high in variance (overfitted).


With the theory explained, we can go on to describe how we have applied it to our problem.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Implementation}
The source code for our implementation can be found here: \href{url}{https://github.com/jonatan-hanssen/3155-project1.git}.
\subsubsection{Preprocessing: Creating the design matrix and scaling our data}
We have two datasets, one synthetic and one real. Regardless of the source of the data, we create the design matrix in a similar manner, as the relation between our input data and our target values are the same for both datasets, that is, for each combination of $x$ and $y$, we have a height $z$. For our real data, this value represents the elevation in meters above ocean level for a terrain in Norway. As we wish to perform a polynomial fit, we create our design matrix such that each row $i$ has the following values:

\begin{equation}
X_{i*} = 1 \;\; x_i \;\; y_i \; x_i^2 \;\; x_i y_i \;\; y_i^2 \;\; ... \;\; x_i^p y_i^p
\end{equation}

Below is the python code used to create the design matrix, which can be found in \texttt{src/utils.py}.

\lstinputlisting[language=Python, firstline=30, lastline=44]{../src/utils.py}

While the creating of the design matrix is equivalent for the two datasets, the preprocessing of the data afterwards is not. For the Franke function no scaling is needed, as we generate the data ourselves to be between $0$ and $1$. Given that we are doing a polynomial fit, every element in our design matrix will be a multiplication of $x^p$ and $y^k$ for different $p$'s and $k$'s, which, given our choice of range for $x$ and $y$, guarantees that every element is between $0$ and $1$. However, we have included a centering of the data, which has no effect on the Ordinary Least Squares regression, but which is used when applying Ridge and Lasso regression, to avoid punishing the intercept. The centering is done as follows: First, the first coloumn of the design matrix (the intercept coloumn) is discarded. Then we calculate the mean for the target values and the design matrix (the mean for the design matrix is a vector with mean values for each coloumn). We subtract this from the design matrix and the target values when performing the fit giving us a vector $\bm{\beta}$. Afterwards we calculate the intercept with the following equation

\begin{equation}
	intercept = \bar{y} - \bm{\bar{x}_c}^T \bm{\beta}
\end{equation}

	where $\bar{y}$ is the mean value of $\bm{y}$ and $\bm{\bar{x}_c}$ is a vector of size $p$ with $\bar{X}_i = mean(X_i)$ (the mean of each coloumn in $X$). We finally create our prediction by using the $\bm{\beta}$, $X$ and intercept we calculated above:

\begin{equation}
	\bm{\hat{y}} = X \bm{\beta} + intercept
\end{equation}

The code for this, taken from the \texttt{evaluate\_model} function in \texttt{src/utils.py} is as follows:\footnote{Some intermediate parts of our implementation not related to this have been removed for readability}

\begin{lstlisting}[language=Python]
X_train = X_train[:, 1:]
z_train_mean = np.mean(z_train, axis=0)
X_train_mean = np.mean(X_train, axis=0)
beta = model((X_train - X_train_mean), (z_train - z_train_mean))
intercept = np.mean(z_train_mean - X_train_mean @ beta)
z_pred_train = X_train @ beta + intercept
\end{lstlisting}

Where \texttt{model} calculates \texttt{beta} using the normal equations for OLS.

For the real elevation data, scaling is definitely required, as the values for $x$, $y$ reach several hundred and $z$ reaches over a thousand. For polynomial degree $10$ and an $x$ value which reaches $400$, we will have a design matrix with values from $0$ to $400^{10}$. This large range of values makes our linear regression perform poorly. We have therefore chosen to scale our design matrix using Scikit's \texttt{MinMaxScaler}, which scales each column to be in the range of $(0,1)$. The scaling coefficients are calculated on the training data, and applied to both the training and test data. We have done the same with the values for $z$. This gives a much more stable model. The code for doing this on the design matrix is as follows, taken from \texttt{src/utils.py}:

\lstinputlisting[language=Python, firstline=242, lastline=249]{../src/utils.py}

Another form of "preprocessing" which is strictly practical in nature, is that we have reduced the dimensions of the data to 320 by 640. This was done by downscaling the image.\footnote{The interpolation was done using "nearest neighbor interpolation", which does not introduce new values into the data \cite[77]{dip}. An equivalent programmatic solution would be to for instance only look at every 4 pixels} This was simply done because the design matrix becomes several gigabytes in size if one uses the whole dataset, and model fitting then takes an unreasonable amount of time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Implementation of linear regression}

Linear regression for OLS and Ridge are implemented by using NumPy's \texttt{numpy.linalg.pinv} and the normal equations. We have implemented them as functions which take in the target values and the design matrix and return $\bm{\hat{\beta}}$. They are called from a wrapper function called \texttt{evaluate\_model} which takes care of the scaling and works with both SciKit models and our own. Below are the functions, taken from \texttt{src/utils.py}:

\lstinputlisting[language=Python, firstline=56, lastline=64]{../src/utils.py}

For repeated calls of these functions with different polynomial degrees, we use the function \texttt{linreg\_to\_N}, which passes the correctly sliced design matrix to the regression functions and saves the predictions and various performance metrics for each polynomial degree for easy plotting.

For Lasso regression, we use SciKit's implementation. We also use SciKit's implementation for OLS and Ridge for comparisons with our own implementation.

The correctness of our Ordinary Least Squares regression can be confirmed by running our debug function (the so called SkrankeFunction)\footnote{this can be done by running \texttt{\$ python3 task\_b.py --debug --noscale -n 2}}, which is a second degree polynomial where $b_0 = 0, \; b_1 = 1, \; b_2 = 2$ and so on. The SkrankeFunction (taken from \texttt{src/utils.py}) looks like this

\lstinputlisting[language=Python, firstline=26, lastline=27]{../src/utils.py}

and the result of our regression is correctly given as $\bm{\beta} = (0 \; 1 \; 2 \; 3 \; 4 \; 5\;)^T$.

\subsubsection{Implementation of resampling methods}

We use resampling methods to better estimate our performance metrics. We use both cross validation and the bootstrap method. Cross validation is implemented in the function \texttt{crossval} in \texttt{src/utils.py}. We split the data into $K$ folds by slicing with NumPy taking consecutive rows of the design matrix and target vector. But before we do this, we resample the data with Scikit's \texttt{resample}. This is important because our $x$'s and $y$'s are not normal distributions but consecutive values between $0$ and $1$, and therefore consecutive rows correspond specific areas of our data, which is not wanted. We then fit on $K-1$ folds, and test on the remaining fold. We return the average error.

We implement bootstrap by repeatedly resampling our data with replacement and performing the prediction. We use the predictions returned from hundreds of resamplings to calculate the bias and variance of our model.

\subsubsection{Franke Function}

We use the Franke Function with an added stochastic noise of to test our models. We explore how higher and lower levels of noise affect the model. Below is the Franke function:

\begin{multline}
f(x,y) = \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4}-\frac{(9y-2)^2}{4}\right)} + \\ \frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}-\frac{(9y+1)}{10}\right)} + \\
\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4}-\frac{(9y-3)^2}{4}\right)} - \\
\frac{1}{5}\exp{\left(-(9x-4)^2-(9y-7)^2\right)}
\end{multline}

As we can see, it is simply a weighted sum of exponentials. As such it has an infinite Taylor expansion into polynomials.\footnote{The proof for this is left as an exercise for the reader}


\section{Results}

\subsection{Resampling}

As stated in the method section, we suspect that bootstrap and cross validation both yield approximations to the true performance metrics due to the central limit theorem, and thus should give identical results. We can observe from Fig.~\ref{frankemsebyresamplingmethod} that our implementation of bootstrap and cross validation yield almost exactly the same results. (Fig.~\ref{realmsebyresamplingmethod} in the appendix shows a similar result for the real data)

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/frankemsebyresamplingmethod.png}}
\caption{Comparison between approximated MSE for different resampling methods for the Franke function}
\label{frankemsebyresamplingmethod}
\end{figure}


\subsection{Franke Function}

To gain insight into how the three different regression methods work, we experiment with synthetic data generated by the Franke function.

\subsubsection{Ordinary Least Squares}

We find that OLS creates a good fit for the Franke Function. With 400 datapoints, we can increase our polynomial degree to 20 before we see signs of overfitting.\footnote{Here we also see something a bit surprising: We see that our own implementation of OLS follows Scikit's implementation up to around polynomial degree 10, at which point scikit's MSE increases rapidly. We chalk this up to numerical errors and differing implementation of the normal equations, as we simply use the normal equation exactly, while scikit uses an implementation which is probably computationally more efficient but gives slight differences when comparing to the normal equation. These differences seem to create a large error after a certain size of the design matrix is reached, which we can not adequately explain.} This is most likely because the Franke function can be expanded into a sum of polynomials. We would expect to see higher polynomial degrees give us better and better prediction, up to a certain point.\footnote{And indeed, this seems to be the case, as if we increase the availiable datapoints considerably, as seen in Fig.~\ref{frankiehigherdatapoints} in the appendix, we can reach much higher polynomial degrees without overfitting} When we reach degrees higher than around 20, we see that our test error increases slightly, a sign of overfitting. This is also to be expected, as even if the continous Franke function could be expanded into a series of polynomials, we are working with a limited dataset of only 400 samples (and only 80\% of those in our training set). As we reach higher and higher degrees, we are more and more likely to see unfortunate train-test-splits where the best fit for the training data leads to large errors for the test data. Regardless, our fit for lower model complexity is remarkably good. See Fig.~\ref{frankenonoise} for a comparison between the Franke function and our best prediction, which shows that they are essentially identical. Fig.~\ref{frankenonoisemse} shows the train and test MSE by polynomial degree. The MSE for test in these plots is the average MSE for 100 bootstraps. For a comprehensive list of the shell command required to reproduce this and every other plot, see Table~\ref{allparamstable} in the appendix.

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/frankenonoise.png}}
\caption{A comparison of our best OLS prediction and the Franke function with no noise, at polynomial degree 10}
\label{frankenonoise}
\end{figure}

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/frankenonoisemse.png}}
\caption{Test and train MSE for the Franke function with no noise, using OLS}
\label{frankenonoisemse}
\end{figure}

However, when we introduce a stochastic noise, we see that overfitting happens rapidly. Without a regularization hyperparameter, parameters fluctuate wildly and our prediction suffers. Fig.~\ref{frankenoisemse} shows our MSE up to polynomial degree 24 with an added stochastic noise $\epsilon \sim N(0,0.05)$, where we see that we are overfitting severely starting at around degree 7. See Fig.~\ref{frankienoisecomparison} in the appendix for test MSE for more and more noisy target values. 

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/frankenoisemse.png}}
\caption{Test and train MSE for the Franke function with noise, using OLS}
\label{frankenoisemse}
\end{figure}

By using bootstrap resampling, we can study the bias-variance-tradeoff, which shows how the variance of our model between different bootstraps iterations increases as our model overfits our noisy data. See Fig.~\ref{frankiebiasvariance}. Clearly, we need to introduce regularization if we want a model that does not overfit. Fig. ~\ref{frankenoisybeta} in the appendix shows the beta progression for selected betas, clearly showing that we are overfitting.

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/frankiebiasvariance.png}}
\caption{Bias, variance and test MSE plotted over polynomial degree for our OLS prediction on the Franke function with an added stochastic noise $\epsilon \sim N(0,0.05)$}
\label{frankiebiasvariance}
\end{figure}

Table ~\ref{ols_mse_table} shows the best polynomial degree and MSE for regular and noisy data. Here we see that the best degree without noise is 9, which shows that even without noise, the low amount of data leads to a slight overfit even at very small polynomial degrees. This is not surprising, as at $N = 9$ we have $55$ features, which is a substantial amount compared to our measly 320 training datapoints. We could of course arbitrarily increase our dataset to ludicrous levels, but doing linear regression with this dataset gives valuable insight into how our performance is dependent on our availiable data.

\begin{table}[h]
\caption{Best MSE and polynomial degree for OLS}
\begin{center}
\label{ols_mse_table}
\begin{tabular}{c l l}
% & & \\ % put some space after the caption
% \hline
Std of noise & Best MSE & Best degree \\
\hline
0 & 0.0005109 & 9\\
0.05 & 0.0038302 & 7 \\
0.10 & 0.0139833 & 7 \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Ridge}

Using Ridge regression, we see from Fig.~\ref{frankiedifflambdas} in the appendix that larger and larger values of $\lambda$ efficiently prevents overfitting and reduces the variance between predictions. Looking at the beta progression for the same values of $\lambda$ (Fig.~\ref{frankeridgebetas} in the appendix), we see that the regularization parameter is successfully shrinking the beta values. Using Scikit's gridsearch, we find the optimal $\lambda$ to be $6.10 \cdot 10^{-8}$ and the optimal polynomial degree to be 13, with an average MSE of $0.003027$. This is a good improvement of our OLS fit, and shows that we are able to push our model complexity significantly higher and gain a moderate MSE improvement with a correctly chosen regularization parameter. Looking at the beta progression for this value of lambda (see Fig.~\ref{frankebestlambdabetas}), we see that the beta values settle and do not vary greatly between polynomial degrees. Fig.~\ref{frankebestlambdamse} shows the MSE score, which shows how we are successfully avoiding overfitting. Table~\ref{ols_vs_ridge_vs_lasso_table} shows the relevant values.

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/frankebestlambdabetas.png}}
\caption{Beta progression for our Ridge prediction with the optimal lambda}
\label{frankebestlambdabetas}
\end{figure}

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/frankebestlambdamse.png}}
\caption{Test and train MSE for ridge}
\label{frankebestlambdamse}
\end{figure}


% We see from this that Ridge regression has not really had much of an effect in decreasing the MSE. We must now remember what Ridge aims to do. With the regularization parameter, we can prevent overfitting if our model is too complex for our data. Looking at Fig.~\ref{frankeridgenoisecomparison} in comparison to Fig.~\ref{frankienoisecomparison} (both in the appendix), we can see that Ridge really has achieved this, reducing overfitting as our data becomes more and more noisy. But we should recall that we decide the complexity of our model, by deciding which polynomial degree we use when creating the design matrix. In fact, by choosing polynomial degree 7 for our OLS method to get the best MSE for noisy data, we are in fact doing a primitive form of best subset selection, as described in Hastie et Al.\cite[57]{hastie}. Almost by definition, the polynomial degree with the best MSE is not overfitted, because if it was, it would not be the model with the lowest MSE among all polynomial degrees. Therefore it is not so unexpected that Ridge regression has a relatively low impact on the MSE. We could expect that Ridge would allow us to reach higher polynomial degrees without overfitting, by shrinking features of the higher polynomial degrees that are not supported but allowing us to keep other high degree terms (for example, allowing us to use $x^5y^4$, but discard $x^9$, if only $x^5y^4$ was supported by the data). But we can see from the Table ~\ref{ols_vs_ridge_vs_lasso_table} that we are not able to push into higher degrees. A possible explanation for this is that every feature is useful, due to the polynomial expansion of the Franke function. 

\subsubsection{Lasso}

Looking at Lasso regression, we find that it has the same capability to reduce overfitting as Ridge, as can be seen in Fig.~\ref{frankedifflambdaslasso} in the appendix. However, looking at Table~\ref{ols_vs_ridge_vs_lasso_table}, we see that Lasso falls short when finding a good MSE. Intuitively, it does not make sense for Lasso to be worse than OLS, because Lasso with $\lambda = 0$ is equivalent to OLS. But we cannot avoid considering the fact that Lasso does not have an analytical solution, and as such we must use numerical methods to find the optimal parameters. Using scikit's implementation, we have not been able to get the numerical method to converge, which means that we have not found a global minimum.\footnote{Scikit explicitly mentions that using $\lambda = 0$ does not converge well with their implementation \cite{lasso}} For this reason, Lasso regression has not given satisfying results.


\begin{table*}[t]
\caption{Best MSE for OLS, Ridge and Lasso with 400 datapoints}
\begin{center}
\label{ols_vs_ridge_vs_lasso_table}
\begin{tabular}{c | l l l l}
% & & \\ % put some space after the caption
% \hline
Regression & Std of noise & MSE & Degree & $\lambda$ \\
\hline
OLS & 0.05 & 0.0038302 & 7 & - \\
Ridge & 0.05 & 0.003027 & 13 & $6.10 \cdot 10^{-8}$ \\
Lasso & 0.05 & 0.008298 & 12 & $1.38 \cdot 10^{-5}$ \\
\hline
\end{tabular}
\end{center}
\end{table*}


\subsection{Real elevation data}

With the confidence that our methods work as expected on synthetic data, we turn our attention to the real elevation data. First, let us confirm that it is necessary to scale our values using MinMax-scaling, as we have described in our implementation. Fig.~\ref{realunscaled} and Fig.~\ref{realunscaledmse} in the appendix shows our predicted terrain and associated MSE, showing that we reach a very poor prediction without scaling.

\subsubsection{Ordinary Least Squares}

Here we find some limitations of linear regression. We see from Fig.~\ref{real1msetraintest} that the we are able to fit the terrain up to a certain point, gaining an MSE of $0.0142683$ with OLS with polynomial degree 19, our highest polynomial. This is about two orders of magnitude higher than our lowest MSE for the Franke function without noise, which had values on a similar scale.\footnote{The Franke function without noise has a range of about $(0,1.2)$ and our MinMaxed terrain data has been scaled to a range of $(0,1)$. While not exactly the same, these are on the same order of magnitude, and we believe that comparing the MSE scores is reasonable} In fact, our MSE is comparable to the Franke function with a stochastic noise $\epsilon \sim N(0,0.1)$ (Table~\ref{ols_mse_table_dif_data}), which implies that our terrain is very irregular and difficult to approximate. Looking at the plot for polynomial degree 19 (Fig.~\ref{real1surfacecomp} in the appendix), we see that our poor MSE is reflected visually in our fit as well, as we can only make out general similarities in shape, not finer details. Comparing the best polynomial degree of the best fit with the synthetic data, we see that our best fit is reached much later. This implies that we are not overfitting yet, and indeed the bias variance decomposition confirms this, as we see that variance is essentially inconsequential (Fig.~\ref{realbiasvarianceOLS}). The fact that we are not overfitting can be seen when we increase the polynomial degree to very large amounts, seeing only a very modest decrease in MSE to $0.01381$ at degree 40 (Fig.~\ref{realOLSmse40} in the appendix). Comparing this to the tenfold decrease in MSE when increasing the polynomial degree for the Franke function to 50, we see a clear difference. Looking at the beta progression for selected betas (Fig.\ref{realOLSbetaprog40} in the appendix), we see that the parameters stabilize, which again implies that we are not overfitting, but simply reaching our best possible prediction for polynomial regression. It is reasonable to expect that our model will not overfit, due to the large increase in data we now use for the fit. Our synthetic data used $20 x 20 = 400$ datapoints, while we now have $320 x 640 = 204 800$ datapoints. This means that for a 19th order polynomial, which will have $210$ features, we will have around a thousand datapoints per feature. If we decrease the number of datapoints availiable considerably, we can see overfitting start to occur (Fig.~\ref{real1mselessdata} in the appendix). So we are not overfitting, but we are not decreasing our MSE either. This is likely because the elevation data has a jagged and complex shape, which cannot be accurately recreated, even with a polynomial of degree $19$. We should remember that the Franke function can be expanded into a series of polynomials, which is why we can expect to get a lower and lower MSE as the polynomial degree increases (so long as we have enough data and it is not noisy). The same is not true for our elevation data, and as such we have little to gain from increasing our model complexity after a certain point. Clearly, linear regression does not offer a very good tradeoff between computational cost and performance for this data.

\begin{table}
\caption{Best MSE and polynomial degree for OLS on synthetic and real data}
\begin{center}
\label{ols_mse_table_dif_data}
\begin{tabular}{c l l l}
Dataset & Std of noise & Best MSE & Best degree \\
\hline
Franke & 0 & 0.0005109 & 9\\
Franke & 0.05 & 0.0038302 & 7 \\
Franke & 0.10 & 0.0139833 & 7 \\
Terrain 1 & - & 0.0142683 & 19 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/realbiasvarianceOLS.png}}
\caption{Bias, variance and test MSE plotted over polynomial degree for our OLS prediction of Terrain 1}
\label{realbiasvarianceOLS}
\end{figure}


% \begin{figure} 
% \centerline{\includegraphics[width=3.25in]{figure/real1msetraintest.png}}
% \caption{Test and train MSE for OLS on Terrain 1}
% \label{real1msetraintest}
% \end{figure}

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/real1msetraintest.png}}
\caption{Test and train MSE for OLS on Terrain 1}
\label{real1msetraintest}
\end{figure}

\subsubsection{Ridge Regression}

As our model is not overfitting, we do not expect Ridge to offer much improvement. This proves to be the case, as we see from Fig.~\ref{real1ridgedifflambdas} in the appendix. Here we see that different values of lambda have very little impact, as there are no parameters that need regularization, and no overfitting to supress. For very large values, we see that we are shrinking the parameters too much, and we see a slight increase in MSE, but overall we do not see much of an effect. We could expect to see that Ridge shows improvement at higher polynomial degrees, when OLS starts to overfit, but due to computational constraints we have not been able to push our grid search much further than 9 degrees. Using scikit's GridSearch, we find that the optimal lambda is 0, which is exactly what we would expect on a model that does not overfit, as Ridge regression with a lambda equal to 0 is equivalent with OLS. See Table~\ref{ols_vs_ridge_vs_lasso_table_real}.

\subsubsection{Lasso Regression}

With lasso regression, we see similar results as with Ridge. Fig.~\ref{real1lassobiasvariance} in the appendix shows little change, except for when our $\lambda$ is too high and we force all parameters to $0$. Comparing lasso to the two other methods for the real dataset has proven to be difficult, as the numerical methods for finding the best parameters are very time consuming. We have not been able to gridsearch up to polynomial degree 19, but up to degree 9 we found the best lambda to be $2.154^{-7}$ for polynomial degree 9, giving an MSE of $0.01685$.

Comparing all three methods up to degree 9 for different lambdas (using the same lambda for Lasso and Ridge, just to get an overview) in Fig.~\ref{real1msecomparison} in the appendix, we see the same trend as we have been describing. Regularization offers little reward, which is to be expected when we are not overfitting.


\begin{table*}[t]
\caption{Best MSE for OLS, Ridge and Lasso regression on real elevation data. We only fit up to degree 9 due to computational constraints}
\begin{center}
\label{ols_vs_ridge_vs_lasso_table_real}
\begin{tabular}{c | l l l}
Regression & MSE & Degree & $\lambda$ \\
\hline
OLS & 0.01538 & 9 & - \\
Ridge & 0.01538 & 9 & 0 \\
Lasso & 0.01685 & 9 & $2.154 \cdot 10^{-7}$ \\
\hline
\end{tabular}
\end{center}
\end{table*}

\section{Conclusion}

In this paper we have compared Ordinary Least Squares, Ridge and Lasso regression for both synthetic and real data. We have seen the value of using resampling methods such as bootstrap and cross validation to gain valuable insights about the quality of our predictions. Using resampling methods, we have found that linear regression was well suited for approximating the synthetic Franke function, but less suited for the real elevation data. We have seen that a correctly chosen regularization parameter can allow us to push our model complexity higher than we can with OLS, and thus give us a better MSE than OLS. Looking at the real data, we have seen that regularization does not offer much value when we are not overfitting our dataset. Thus all three linear regression methods perform similarly on this dataset. 

\bibliographystyle{apalike}
\bibliography{bibliography}

\section*{Appendix A: Plots and tables}

The next pages contain plots and tables that are of interest.

\begin{table*}
\caption{This table shows the command to execute to reproduce every figure in the report. (More info about the scripts and their parameters can be found in the README)}
\begin{center}
\label{allparamstable}
\begin{tabular}{c | l l l}
Figure & Shell command (leading \texttt{python3} omitted) \\
\hline
\ref{frankenonoise} & \texttt{task\_b.py --noise 0 -n 25}\\
\ref{frankebestlambdamse} & \texttt{task\_b.py --noise 0 -n 25} \footnotesize{(\it Model has been changed to ridge with lambda=$1.43 \cdot 10^{-7}$)}\\
\ref{frankebestlambdabetas} & \texttt{task\_b.py --noise 0 -n 25} \footnotesize{(\it Model has been changed to ridge with lambda=$1.43 \cdot 10^{-7}$)}\\
\ref{realunscaled} & \texttt{task\_b.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 19 --noscale}\\
\ref{realunscaledmse} & \texttt{task\_b.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 19 --noscale}\\
\ref{frankenonoisemse} & \texttt{task\_c.py --noise 0 -n 24}\\
\ref{frankenoisemse} & \texttt{task\_c.py -n 24}\\
\ref{frankiebiasvariance} & \texttt{task\_c.py -n 24 --step 0.1}\\
\ref{frankiedifflambdas} & \texttt{task\_e\_biasvariance.py -n 24 --step 0.1}\\
\ref{frankedifflambdaslasso} & \texttt{task\_f\_biasvariance.py -n 9 --step 0.15}\\
\ref{real1ridgedifflambdas} & \texttt{task\_e\_biasvariance.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 24}\\
\ref{frankeridgebetas} & \texttt{task\_e\_betas.py -n 24}\\
\ref{frankenoisybeta} & \texttt{task\_c.py -n 24}\\
\ref{frankienoisecomparison} & \texttt{noise\_plot.py}\\
\ref{frankiehigherdatapoints} & \texttt{task\_c.py --noise 0 -n 30 --step 0.01}\\
\ref{real1surfacecomp} & \texttt{task\_b.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 19}\\
\ref{real1mselessdata} & \texttt{task\_c.py --file ../data/tiny\_SRTM\_data\_Norway\_1.tif -n 19}\\
\ref{real1msetraintest} & \texttt{task\_c.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 19}\\
\ref{realbiasvarianceOLS} & \texttt{task\_c.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 19}\\
\ref{realOLSmse40} & \texttt{task\_b.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 40}\\
\ref{realOLSbetaprog40} & \texttt{task\_b.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 40}\\
\ref{frankemsebyresamplingmethod} & \texttt{task\_d.py -n 15 --step 0.02}\\
\ref{real1msecomparison} & \texttt{task\_f\_msecomparison.py --file ../data/small\_SRTM\_data\_Norway\_1.tif -n 9}\\
\hline
\end{tabular}
\end{center}
\end{table*}

%%%%%%%%%%% FIGURES %%%%%%%%%%%%%%%%%%%%%%%%%5

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/realmsebyresamplingmethod.png}}
\caption{Comparison between approximated MSE for different resampling methods for the real terrain data}
\label{realmsebyresamplingmethod}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/frankiehigherdatapoints.png}}
\caption{Franke function fitted up to polynomial degree 30, with 10000 datapoints instead of 400. As we can see, we do not overfit even at this polynomial degree when the availiable data is plentiful. The highest MSE was reached at $N=29$, and was $0.00003511$, one tenth of our best prediction with only 400 datapoints}
\label{frankiehigherdatapoints}
\end{figure*}

\begin{figure*}
% \begin{figure}[t]
\centerline{\includegraphics[width=3.25in]{figure/frankienoisecomparison.png}}
\caption{Test and train MSE for the OLS prediction of Franke function with an increasing amount of noise}
\label{frankienoisecomparison}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/frankenoisybeta.png}}
\caption{Beta progression for OLS}
\label{frankenoisybeta}
\end{figure*}

\begin{figure*} 
\centerline{\includegraphics[width=3.25in]{figure/frankiedifflambdas.png}}
\caption{Bias, variance and test MSE plotted over polynomial degree for our Ridge prediction on the Franke function with an added stochastic noise $\epsilon \sim N(0,0.05)$, using lambdas of increasing size. Here we have reduced the number of datapoints to only 100 to force overfitting earlier.}
\label{frankiedifflambdas}
\end{figure*}

\begin{figure*} 
\centerline{\includegraphics[width=3.25in]{figure/frankeridgebetas.png}}
\caption{Beta progression for different values of lambda in Ridge regression. Note the different scales on each of the plots, showing how increasing the regularization parameter shrinks the betas}
\label{frankeridgebetas}
\end{figure*}

% \begin{figure*}
% \centerline{\includegraphics[width=3.25in]{figure/frankeridgenoisecomparison.png}}
% \caption{Test and train MSE for the Ridge prediction with $\lambda = 10^{-8}$ of the Franke function with an increasing amount of noise}
% \label{frankeridgenoisecomparison}
% \end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/frankedifflambdaslasso.png}}
\caption{Bias, variance and test MSE plotted over polynomial degree for our Lasso prediction on the Franke function with an added stochastic noise $\epsilon \sim N(0,0.05)$, using lambdas of increasing size. Here we have reduced the number of datapoints to less than 100 to force overfitting earlier.}
\label{frankedifflambdaslasso}
\end{figure*}





\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/realunscaled.png}}
\caption{Our prediction for the real terrain using unscaled data, showing that we reach a very poor result}
\label{realunscaled}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/realunscaledmse.png}}
\caption{Test and train MSE for our prediction for the real terrain using unscaled data, showing that we reach a very poor result}
\label{realunscaledmse}
\end{figure*}




\begin{figure*} 
\centerline{\includegraphics[width=6.85in]{figure/real1surfacecomp.png}}
\caption{A comparison of our best OLS prediction and actual terrain data (terrain 1), at polynomial degree 19}
\label{real1surfacecomp}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/realOLSmse40.png}}
\caption{Test and train MSE on our terrain, this time up to degree 40. We see only a slight decrease in MSE, down to $0.01381$, even though we double the amount of degrees. This has not been bootstrapped due to computational limitations}
\label{realOLSmse40}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/realOLSbetaprog40.png}}
\caption{Beta progression up to polynomial degree 40 for real terrain data, showing that the are relatively constant, implying that we are not overfitting}
\label{realOLSbetaprog40}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/real1mselessdata.png}}
\caption{Test and train MSE for the OLS prediction of terrain 1, this time downscaled to only $50x100$. Here the optimal degree was 11, showing that with lower data we start to overfit more rapidly.}
\label{real1mselessdata}
\end{figure*}

\begin{figure} 
\centerline{\includegraphics[width=3.25in]{figure/real1ridgedifflambdas.png}}
\caption{Bias variance tradeoff for ridge regression with different lambdas, for the real dataset}
\label{real1ridgedifflambdas}
\end{figure}

\begin{figure} 
\centerline{\includegraphics[width=3.25in]{figure/real1lassobiasvariance.png}}
\caption{Bias variance tradeoff for lasso regression with different lambdas, for the real dataset}
\label{real1lassobiasvariance}
\end{figure}

\begin{figure*}
\centerline{\includegraphics[width=3.25in]{figure/real1msecomparison.png}}
\caption{Comparison between OLS, Ridge and Lasso for different values of lambda, for real terrain data}
\label{real1msecomparison}
\end{figure*}





\end{document}
